{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamAdebisi/Stable_Diffusion/blob/main/deep/deep_dive2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bR6HLcKlWkIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa66362-806c-460d-938e-bd5e06ba7166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade transformers==4.36.2 diffusers ftfy accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGnqzZxWW8Fv"
      },
      "outputs": [],
      "source": [
        "from base64 import b64encode\n",
        "\n",
        "import numpy\n",
        "import torch\n",
        "from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# For video display:\n",
        "from IPython.display import HTML\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch import autocast\n",
        "from torchvision import transforms as tfms\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPTextModel, CLIPTokenizer, logging\n",
        "import os\n",
        "\n",
        "torch.manual_seed(1)\n",
        "if not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()\n",
        "\n",
        "# Supress some unnecessary warnings when loading the CLIPTextModel\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# Set device\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "if \"mps\" == torch_device: os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZGpjpfgYLBt"
      },
      "outputs": [],
      "source": [
        "# pip uninstall transformers -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glHJpYRuY9F3"
      },
      "outputs": [],
      "source": [
        "# pip install transformers==4.36.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "NK_OgxLsZFLi",
        "outputId": "eb56f9bc-eb37-459f-f125-9e05f0f94216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.36.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.32.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers) (11.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers) (1.1.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.22.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2025.4.26)\n",
            "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.36.2\n",
            "    Uninstalling transformers-4.36.2:\n",
            "      Successfully uninstalled transformers-4.36.2\n",
            "Successfully installed tokenizers-0.21.1 transformers-4.52.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "7457461967f94606a782f5662e201f01"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install --upgrade diffusers transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1hFr4OsaDrF"
      },
      "outputs": [],
      "source": [
        "# Load the autoencoder model which will be used to decode the latents into image space.\n",
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
        "\n",
        "# Load the tokenizer and text encoder to tokenize and encode the text.\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# The UNet model for generating the latents.\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
        "\n",
        "# The noise scheduler\n",
        "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "\n",
        "# To the GPU we go!\n",
        "vae = vae.to(torch_device)\n",
        "text_encoder = text_encoder.to(torch_device)\n",
        "unet = unet.to(torch_device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHF9uZJVbfLr"
      },
      "outputs": [],
      "source": [
        "# Some settings\n",
        "prompt = [\"A watercolor painting of an otter\"]\n",
        "height = 512                        # default height of Stable Diffusion\n",
        "width = 512                         # default width of Stable Diffusion\n",
        "num_inference_steps = 30            # Number of denoising steps\n",
        "guidance_scale = 7.5                # Scale for classifier-free guidance\n",
        "generator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\n",
        "batch_size = 1\n",
        "\n",
        "# Prep text\n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "# Prep Scheduler\n",
        "def set_timesteps(scheduler, num_inference_steps):\n",
        "    scheduler.set_timesteps(num_inference_steps)\n",
        "    scheduler.timesteps = scheduler.timesteps.to(torch.float32) # minor fix to ensure MPS compatibility, fixed in diffusers PR 3925\n",
        "\n",
        "set_timesteps(scheduler,num_inference_steps)\n",
        "\n",
        "# Prep latents\n",
        "latents = torch.randn(\n",
        "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "  generator=generator,\n",
        ")\n",
        "latents = latents.to(torch_device)\n",
        "latents = latents * scheduler.init_noise_sigma # Scaling (previous versions did latents = latents * self.scheduler.sigmas[0]\n",
        "\n",
        "# Loop\n",
        "with autocast(\"cuda\"):  # will fallback to CPU if no CUDA; no autocast for MPS\n",
        "    for i, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n",
        "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "        latent_model_input = torch.cat([latents] * 2)\n",
        "        sigma = scheduler.sigmas[i]\n",
        "        # Scale the latents (preconditioning):\n",
        "        # latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5) # Diffusers 0.3 and below\n",
        "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "        # predict the noise residual\n",
        "        with torch.no_grad():\n",
        "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "        # perform guidance\n",
        "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "        # compute the previous noisy sample x_t -> x_t-1\n",
        "        # latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"] # Diffusers 0.3 and below\n",
        "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "# scale and decode the image latents with vae\n",
        "latents = 1 / 0.18215 * latents\n",
        "with torch.no_grad():\n",
        "    image = vae.decode(latents).sample\n",
        "\n",
        "# Display\n",
        "image = (image / 2 + 0.5).clamp(0, 1)\n",
        "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "images = (image * 255).round().astype(\"uint8\")\n",
        "pil_images = [Image.fromarray(image) for image in images]\n",
        "pil_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUBjYAIXeiJX"
      },
      "outputs": [],
      "source": [
        "def pil_to_latent(input_im):\n",
        "    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n",
        "    with torch.no_grad():\n",
        "        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n",
        "    return 0.18215 * latent.latent_dist.sample()\n",
        "\n",
        "def latents_to_pil(latents):\n",
        "    # bath of latents -> list of images\n",
        "    latents = (1 / 0.18215) * latents\n",
        "    with torch.no_grad():\n",
        "        image = vae.decode(latents).sample\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "    images = (image * 255).round().astype(\"uint8\")\n",
        "    pil_images = [Image.fromarray(image) for image in images]\n",
        "    return pil_images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAdNdO-iwT1n"
      },
      "outputs": [],
      "source": [
        "# Download a demo Image\n",
        "!curl --output macaw.jpg 'https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKuum1NOwT1n"
      },
      "outputs": [],
      "source": [
        "# Load the image with PIL\n",
        "input_image = Image.open('macaw.jpg').resize((512, 512))\n",
        "input_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeLIqF3gwT1n"
      },
      "outputs": [],
      "source": [
        "# Encode to the latent space\n",
        "encoded = pil_to_latent(input_image)\n",
        "encoded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEtImLNZwT1n"
      },
      "outputs": [],
      "source": [
        "# Let's visualize the four channels of this latent representation:\n",
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "for c in range(4):\n",
        "    axs[c].imshow(encoded[0][c].cpu(), cmap='Greys')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJQs4UDUwT1n"
      },
      "outputs": [],
      "source": [
        "# Decode this latent representation back into an image\n",
        "decoded = latents_to_pil(encoded)[0]\n",
        "decoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtbREOgbwT1o"
      },
      "source": [
        "# Setting the number of sampling steps:\n",
        "set_timesteps(scheduler, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcLO08YfwT1o"
      },
      "outputs": [],
      "source": [
        "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ3TUjH1wT1o"
      },
      "outputs": [],
      "source": [
        "scheduler.set_timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUyWsCOMwT1o"
      },
      "outputs": [],
      "source": [
        "# See these in terms of the original 1000 steps used for training:\n",
        "print(scheduler.timesteps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMGXatC3wT1o"
      },
      "outputs": [],
      "source": [
        "# Look at the equivalent noise levels:\n",
        "print(scheduler.sigmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jCu7QP6wT1o"
      },
      "outputs": [],
      "source": [
        "# Plotting this noise schedule:\n",
        "plt.plot(scheduler.sigmas)\n",
        "plt.title('Noise Schedule')\n",
        "plt.xlabel('Sampling step')\n",
        "plt.ylabel('sigma')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0bH4b_gwT1o"
      },
      "outputs": [],
      "source": [
        "noise = torch.randn_like(encoded) # Random noise\n",
        "sampling_step = 10 # Equivalent to step 10 out of 15 in the schedule above\n",
        "# encoded_and_noised = scheduler.add_noise(encoded, noise, timestep) # Diffusers 0.3 and below\n",
        "encoded_and_noised = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[sampling_step]]))\n",
        "latents_to_pil(encoded_and_noised.float())[0] # Display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMoDENbPwT1o"
      },
      "outputs": [],
      "source": [
        "noised_latents = scheduler.add_noise(latents, noise, timesteps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PLF1w1SwT1o"
      },
      "outputs": [],
      "source": [
        "noisy_samples = original_samples + noise * sigmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJPe7RaMwT1p"
      },
      "outputs": [],
      "source": [
        "latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1swk04FtwT1p"
      },
      "outputs": [],
      "source": [
        "latent_model_input = scheduler.scale_model_input(latent_model_input, t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJXggQAVwT1p"
      },
      "outputs": [],
      "source": [
        "# Settings (same as before except for the new prompt)\n",
        "prompt = [\"A colorful dancer, nat geo photo\"]\n",
        "height = 512                        # default height of Stable Diffusion\n",
        "width = 512                         # default width of Stable Diffusion\n",
        "num_inference_steps = 50            # Number of denoising steps\n",
        "guidance_scale = 8                  # Scale for classifier-free guidance\n",
        "generator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\n",
        "batch_size = 1\n",
        "\n",
        "# Prep text (same as before)\n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "# Prep Scheduler (setting the number of inference steps)\n",
        "set_timesteps(scheduler, num_inference_steps)\n",
        "\n",
        "# Prep latents (noising appropriately for start_step)\n",
        "start_step = 10\n",
        "start_sigma = scheduler.sigmas[start_step]\n",
        "noise = torch.randn_like(encoded)\n",
        "latents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n",
        "latents = latents.to(torch_device).float()\n",
        "\n",
        "# Loop\n",
        "for i, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n",
        "    if i >= start_step: # << This is the only modification to the loop we do\n",
        "\n",
        "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "        latent_model_input = torch.cat([latents] * 2)\n",
        "        sigma = scheduler.sigmas[i]\n",
        "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "        # predict the noise residual\n",
        "        with torch.no_grad():\n",
        "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "        # perform guidance\n",
        "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "        # compute the previous noisy sample x_t -> x_t-1\n",
        "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "latents_to_pil(latents)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjO0uqzxwT1p"
      },
      "outputs": [],
      "source": [
        "# Our text prompt\n",
        "prompt = 'A picture of a puppy'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LmDzzgdwT1p"
      },
      "outputs": [],
      "source": [
        "# Turn the text into a sequnce of tokens:\n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "text_input['input_ids'][0] # View the tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGDPn6p4wT1p"
      },
      "outputs": [],
      "source": [
        "# See the individual tokens\n",
        "for t in text_input['input_ids'][0][:8]: # We'll just look at the first 7 to save you from a wall of '<|endoftext|>'\n",
        "    print(t, tokenizer.decoder.get(int(t)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-kM4aA3wT1p"
      },
      "outputs": [],
      "source": [
        "# Grab the output embeddings\n",
        "output_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "print('Shape:', output_embeddings.shape)\n",
        "output_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb51sKGMwT1p"
      },
      "outputs": [],
      "source": [
        "text_encoder.text_model.embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3YFoc-kwT1q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpKhhLh-wT1q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL6km5M0wT1q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzriUNhVwT1q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy4JstHuwT1q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVtE3KzxwT1q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Hn4b227wT1q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtxAckISwT1q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lLCUsFgwT1q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}